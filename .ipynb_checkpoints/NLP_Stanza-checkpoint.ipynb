{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8daad547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6abe9d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "814ccf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stanfordnlp\n",
    "import stanza\n",
    "#stanza.download('en') # download English model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57e81c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61a4f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#txt = \"DISASTROUS! There were 8 of us and after waiting 1 hour and a half, they forgot our command. When it finally arrived, half the burgers were cold and inedible, and the bread was breaking (reheated) and we had to send them back. The fries were dry, refried, and oily. In addition, they were wrong in the companions.\\nThe service was relatively friendly, although the Asian-looking waiter hesitated at my friend and was extremely unprofessional.\\n\\nWhoever takes a lot of space, the less he tightens up!!\"\n",
    "txt = \"The Sound Quality is great but the battery life is very bad.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1405a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = txt.lower()\n",
    "sentList = nltk.sent_tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "055f2009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('sound', 'NN'), ('quality', 'NN'), ('is', 'VBZ'), ('great', 'JJ'), ('but', 'CC'), ('the', 'DT'), ('battery', 'NN'), ('life', 'NN'), ('is', 'VBZ'), ('very', 'RB'), ('bad', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for line in sentList:\n",
    "    txt_list = nltk.word_tokenize(line)\n",
    "    taggedList = nltk.pos_tag(txt_list)\n",
    "print(taggedList)\n",
    "#Output: [('the', 'DT'), ('sound', 'NN'), ('quality', 'NN'), ('is', 'VBZ'), ('great', 'JJ'), ('but', 'CC'), ('the', 'DT'), ('battery', 'NN'), ('life', 'NN'), ('is', 'VBZ'), ('very', 'RB'), ('bad', 'JJ'), ('.', '.')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c0c78d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "    fcluster = []\n",
    "    totalfeatureList = []\n",
    "    finalcluster = []\n",
    "    dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dbb558d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the soundquality is great but the batterylife is very bad .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "newwordList = []\n",
    "flag = 0\n",
    "for i in range(0,len(taggedList)-1):\n",
    "    if(taggedList[i][1]==\"NN\" and taggedList[i+1][1]==\"NN\"):\n",
    "        newwordList.append(taggedList[i][0]+taggedList[i+1][0])\n",
    "        flag=1\n",
    "    else:\n",
    "        if(flag==1):\n",
    "            flag=0\n",
    "            continue\n",
    "        newwordList.append(taggedList[i][0])\n",
    "        if(i==len(taggedList)-2):\n",
    "            newwordList.append(taggedList[i+1][0])\n",
    "finaltxt = ' '.join(word for word in newwordList)\n",
    "print(finaltxt)\n",
    "#Output: the soundquality is great but the batterylife is very bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3529dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "new_txt_list = nltk.word_tokenize(finaltxt)\n",
    "wordsList = [w for w in new_txt_list if not w in stop_words]\n",
    "taggedList = nltk.pos_tag(wordsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "455a205a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c47caa5a9d74e5b82d1308649c370d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-06 10:13:54 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2022-09-06 10:13:54 INFO: Use device: cpu\n",
      "2022-09-06 10:13:54 INFO: Loading: tokenize\n",
      "2022-09-06 10:13:54 INFO: Loading: pos\n",
      "2022-09-06 10:13:54 INFO: Loading: lemma\n",
      "2022-09-06 10:13:55 INFO: Loading: depparse\n",
      "2022-09-06 10:13:55 INFO: Loading: sentiment\n",
      "2022-09-06 10:13:55 INFO: Loading: constituency\n",
      "2022-09-06 10:13:56 INFO: Loading: ner\n",
      "2022-09-06 10:13:57 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n",
      "0 2\n",
      "1 4\n",
      "0 2\n",
      "1 4\n",
      "2 4\n",
      "0 2\n",
      "1 4\n",
      "2 4\n",
      "3 0\n",
      "0 2\n",
      "1 4\n",
      "2 4\n",
      "3 0\n",
      "4 10\n",
      "0 2\n",
      "1 4\n",
      "2 4\n",
      "3 0\n",
      "4 10\n",
      "5 7\n",
      "0 2\n",
      "1 4\n",
      "2 4\n",
      "3 0\n",
      "4 10\n",
      "5 7\n",
      "6 10\n",
      "0 2\n",
      "1 4\n",
      "2 4\n",
      "3 0\n",
      "4 10\n",
      "5 7\n",
      "6 10\n",
      "7 10\n",
      "0 2\n",
      "1 4\n",
      "2 4\n",
      "3 0\n",
      "4 10\n",
      "5 7\n",
      "6 10\n",
      "7 10\n",
      "8 10\n",
      "0 2\n",
      "1 4\n",
      "2 4\n",
      "3 0\n",
      "4 10\n",
      "5 7\n",
      "6 10\n",
      "7 10\n",
      "8 10\n",
      "9 4\n",
      "0 2\n",
      "1 4\n",
      "2 4\n",
      "3 0\n",
      "4 10\n",
      "5 7\n",
      "6 10\n",
      "7 10\n",
      "8 10\n",
      "9 4\n",
      "10 4\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n",
    "doc = nlp(finaltxt)\n",
    "dep_node = []\n",
    "for dep_edge in doc.sentences[0].dependencies:\n",
    "    dep_node.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])\n",
    "    #print(dep_node)\n",
    "    for i in range(0, len(dep_node)):\n",
    "        print(i,int(dep_node[i][1]))\n",
    "        if (int(dep_node[i][1]) != 0):\n",
    "            None\n",
    "            #dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]\n",
    "#print(dep_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "705e718c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['soundquality', 'NN'], ['great', 'JJ'], ['batterylife', 'NN'], ['bad', 'JJ']]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'categoriesList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         categories\u001b[38;5;241m.\u001b[39mappend(i[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(featureList)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcategoriesList\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'categoriesList' is not defined"
     ]
    }
   ],
   "source": [
    "featureList = []\n",
    "categories = []\n",
    "for i in taggedList:\n",
    "    if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "        featureList.append(list(i))\n",
    "        totalfeatureList.append(list(i)) # This list will store all the features for every sentence\n",
    "        categories.append(i[0])\n",
    "print(featureList)\n",
    "print(categoriesList)\n",
    "#Output: [['soundquality', 'NN'], ['great', 'JJ'], ['batterylife', 'NN'], ['bad', 'JJ']]\n",
    "#Output: ['soundquality', 'great', 'batterylife', 'bad']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d39fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = []\n",
    "for i in featureList:\n",
    "    filist = []\n",
    "    for j in dep_node:\n",
    "        if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "            if(j[0]==i[0]):\n",
    "                filist.append(j[1])\n",
    "            else:\n",
    "                filist.append(j[0])\n",
    "    fcluster.append([i[0], filist])\n",
    "print(fcluster)\n",
    "#Output: [['soundquality', ['great']], ['great', ['soundquality']], ['batterylife', ['bad']], ['bad', ['batterylife', 'very']]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9edca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalcluster = []\n",
    "dic = {}\n",
    "for i in featureList:\n",
    "    dic[i[0]] = i[1]\n",
    "for i in fcluster:\n",
    "    if(dic[i[0]]==\"NN\"):\n",
    "        finalcluster.append(i)\n",
    "print(finalcluster)\n",
    "#Output: [['soundquality', ['great']], ['batterylife', ['bad']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2d4405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd42053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "newwordList = []\n",
    "flag = 0\n",
    "for i in range(0,len(taggedList)-1):\n",
    "    if(taggedList[i][1]==\"NN\" and taggedList[i+1][1]==\"NN\"):\n",
    "        newwordList.append(taggedList[i][0]+taggedList[i+1][0])\n",
    "        flag=1\n",
    "    else:\n",
    "        if(flag==1):\n",
    "            flag=0\n",
    "            continue\n",
    "        newwordList.append(taggedList[i][0])\n",
    "        if(i==len(taggedList)-2):\n",
    "            newwordList.append(taggedList[i+1][0])\n",
    "finaltxt = ' '.join(word for word in newwordList)\n",
    "print(finaltxt)\n",
    "#Output: the soundquality is great but the batterylife is very bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f87b95f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb43f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721dc42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline\n",
    "\n",
    "doc = nlp(finaltxt)\n",
    "dep_node = []\n",
    "for dep_edge in doc.sentences[0].dependencies:\n",
    "    dep_node.append([dep_edge[2].text, dep_edge[0].id, dep_edge[1]])\n",
    "    for i in range(0, len(dep_node)):\n",
    "        if (int(dep_node[i][1]) != 0):\n",
    "            dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]\n",
    "print(dep_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd3c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(finaltxt) # Object of Stanford NLP Pipeleine      \n",
    "dep_node = []\n",
    "for dep_edge in doc.sentences[0].dependencies:\n",
    "    dep_node.append([dep_edge[2].text, dep_edge[0].index, dep_edge[1]])\n",
    "    for i in range(0, len(dep_node)):\n",
    "        if (int(dep_node[i][1]) != 0):\n",
    "            dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
